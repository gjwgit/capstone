Capstone Machine Learning
=========================

This machine learning methodology is being developed by Tony Nolan.

Capstone is based on the idea that variables having a common/similar
focus can be grouped together to have greater impact on a model than
they would as separate inputs. Variables are then re-scaled and are
represented in groups. The groups have the effect of maintaining or
increasing their contribution to the model. When the process is
applied in a hierarchical manner, the model then requires less time
and resources to run, because it can achieve the same or better
results using fewer variables.

The Steps in the Process
------------------------

* The Capstone Technique: This is designed to construct an inclusive
  parent dataset, which includes and replicates measurement data and
  descriptive data, in various forms and combinations. The process
  takes all forms of data at all sublevels, and converts them into
  representative values which cascade up to the parent level.

* Data Sequencing: This method applies to the follow on from the
  Capstone technique, where a specific number of measurements are
  recalculated relative to a descriptive value. This process is
  repeated to a number of other descriptive variables, and then
  chained together like a DNA sequence, to map variations within the
  data chains which describe the subject.

* Cohort Analysis: This uses the output from the Data Sequencing, to
  include a subject into a number of different cohorts with other like
  subjects. Subjects are grouped in any number of like cohorts, in any
  combinations, demographics or behaviours to establish a membership
  of a subject into a footprint.

* Foot Printing: This follows on from Cohort Analysis method, where a
  subjectâ€™s data sequence is compared relative to all possible
  combinations, to establish nearest neighbours, and to identify where
  there should be a footprint, but it is missing any population.


System of Systems Engineering
-----------------------------

By using a System of Systems Engineering approach, is some thing like
using a model of models approach, or a dataset of datasets
approach. It uses a parent / child hierarchy within a fuzzy logic
framework, which also incorporates subject frameworks, peer relativity
transformations and digital hash's.  This breaks down Big Data sets
into smaller and much more manageable sub-datasets. By linking these
datasets together in a representative and meaningful way. Child
datasets are be combined and compressed into digital hashes, which
becomes a member of the parent dataset. This is then repeated upwards,
until it reaches the capstone dataset,

Because this process is modular, when you add in cohorts (combinations
of single or multiple variables), then it becomes possible to make a
cohort -cluster-stratification also a system. Which gives a very
powerful tool for making the capstone dataset. When you make the
Capstone dataset in inclusive analytics mode rather than exclusive
analytics mode, you can then use the dataset in variety of different
software.

